# ü§ñ Modelos de Groq Disponibles

## Modelos Soportados

El backend AstroTech soporta 4 modelos de Groq con configuraciones optimizadas:

---

### 1. **Llama 3.3 70B Versatile** (Default)

**Nombre del modelo:** `llama-3.3-70b-versatile`

**Caracter√≠sticas:**
- ‚úÖ Balanceado entre velocidad y calidad
- ‚úÖ Respuestas consistentes
- ‚úÖ R√°pido (~3-4 segundos)
- ‚úÖ Ideal para uso general

**Configuraci√≥n:**
```env
GROQ_MODEL=llama-3.3-70b-versatile
```

**Par√°metros:**
- Temperature: `0.4` (m√°s consistente)
- Max tokens: `1000`
- Response format: JSON object

**Cu√°ndo usarlo:**
- Producci√≥n general
- Respuestas predecibles
- Velocidad importante

---

### 2. **Llama 4 Scout 17B** (M√°s Reciente)

**Nombre del modelo:** `meta-llama/llama-4-scout-17b-16e-instruct`

**Caracter√≠sticas:**
- ‚ú® Modelo m√°s reciente de Meta
- ‚ú® M√°s creativo y variado
- ‚ú® Respuestas √∫nicas
- ‚ö° R√°pido (modelo m√°s peque√±o)

**Configuraci√≥n:**
```env
GROQ_MODEL=meta-llama/llama-4-scout-17b-16e-instruct
```

**Par√°metros:**
- Temperature: `1.0` (m√°s creativo)
- Max tokens: `1024`
- Response format: JSON object

**Cu√°ndo usarlo:**
- Quieres respuestas m√°s creativas
- Predicciones m√°s variadas
- Testing de nuevas capacidades

---

### 3. **Kimi K2 Instruct** (Mayor Capacidad)

**Nombre del modelo:** `moonshotai/kimi-k2-instruct-0905`

**Caracter√≠sticas:**
- üöÄ Mayor capacidad de tokens (4096)
- üöÄ Respuestas m√°s detalladas
- üöÄ Mejor para contenido largo
- ‚öñÔ∏è Balance creatividad/consistencia

**Configuraci√≥n:**
```env
GROQ_MODEL=moonshotai/kimi-k2-instruct-0905
```

**Par√°metros:**
- Temperature: `0.6` (balanceado)
- Max tokens: `4096` (4x m√°s que default)
- Response format: JSON object

**Cu√°ndo usarlo:**
- Necesitas respuestas m√°s extensas
- Predicciones muy detalladas
- Contenido rico en informaci√≥n

---

### 4. **Qwen 3 32B** (Razonamiento Avanzado) ‚ú®

**Nombre del modelo:** `qwen/qwen3-32b`

**Caracter√≠sticas:**
- üß† Capacidades de razonamiento avanzado
- üß† An√°lisis m√°s profundo
- üß† Alta capacidad de tokens (4096)
- ‚öñÔ∏è Balance entre velocidad y calidad

**Configuraci√≥n:**
```env
GROQ_MODEL=qwen/qwen3-32b
```

**Par√°metros:**
- Temperature: `0.6` (balanceado)
- Max tokens: `4096`
- Response format: JSON object

**Cu√°ndo usarlo:**
- Necesitas an√°lisis m√°s profundo
- Predicciones con razonamiento
- Respuestas bien fundamentadas
- Contenido t√©cnico detallado

---

## üîÑ C√≥mo Cambiar de Modelo

### Paso 1: Editar .env

```env
# Cambiar esta l√≠nea:
GROQ_MODEL=llama-3.3-70b-versatile

# Por uno de estos:
GROQ_MODEL=meta-llama/llama-4-scout-17b-16e-instruct
# o
GROQ_MODEL=moonshotai/kimi-k2-instruct-0905
# o
GROQ_MODEL=qwen/qwen3-32b
```

### Paso 2: Reiniciar Servidor

```bash
# Detener servidor (Ctrl+C)
# Iniciar de nuevo:
npm run dev
```

### Paso 3: Probar

```powershell
Invoke-WebRequest -Uri http://localhost:3000/api/oracle `
  -Method POST `
  -ContentType "application/json" `
  -Body '{"sign":"aries"}' | 
  Select-Object -ExpandProperty Content
```

---

## üìä Comparaci√≥n de Modelos

| Caracter√≠stica | Llama 3.3 70B | Llama 4 Scout | Kimi K2 | Qwen 3 32B |
|---------------|---------------|---------------|---------|------------|
| **Velocidad** | ‚ö°‚ö°‚ö° R√°pido | ‚ö°‚ö°‚ö°‚ö° Muy r√°pido | ‚ö°‚ö° Moderado | ‚ö°‚ö°‚ö° R√°pido |
| **Creatividad** | ‚≠ê‚≠ê Baja | ‚≠ê‚≠ê‚≠ê‚≠ê Alta | ‚≠ê‚≠ê‚≠ê Media-Alta | ‚≠ê‚≠ê‚≠ê Media-Alta |
| **Consistencia** | ‚≠ê‚≠ê‚≠ê‚≠ê Alta | ‚≠ê‚≠ê Baja | ‚≠ê‚≠ê‚≠ê Media | ‚≠ê‚≠ê‚≠ê Media |
| **Razonamiento** | ‚≠ê‚≠ê B√°sico | ‚≠ê‚≠ê‚≠ê Bueno | ‚≠ê‚≠ê‚≠ê Bueno | ‚≠ê‚≠ê‚≠ê‚≠ê Excelente |
| **Max Tokens** | 1000 | 1024 | 4096 | 4096 |
| **Temperature** | 0.4 | 1.0 | 0.6 | 0.6 |
| **Uso recomendado** | Producci√≥n | Testing/Creativo | Detallado | An√°lisis/Razonamiento |

---

## üéØ Recomendaciones

### Para Producci√≥n:
```env
GROQ_MODEL=llama-3.3-70b-versatile
```
- Respuestas consistentes
- Velocidad √≥ptima
- Predecible

### Para Experimentar:
```env
GROQ_MODEL=meta-llama/llama-4-scout-17b-16e-instruct
```
- Respuestas m√°s variadas
- M√°s creativo
- Modelo m√°s reciente

### Para Contenido Extenso:
```env
GROQ_MODEL=moonshotai/kimi-k2-instruct-0905
```
- Respuestas m√°s largas
- M√°s detallado
- Mayor capacidad

### Para An√°lisis Profundo:
```env
GROQ_MODEL=qwen/qwen3-32b
```
- Razonamiento avanzado
- An√°lisis t√©cnico
- Respuestas fundamentadas

---

## üîß Configuraci√≥n Avanzada

Si quieres agregar un modelo personalizado, edita `src/services/groq.service.js`:

```javascript
const MODEL_CONFIGS = {
  // Agregar nuevo modelo aqu√≠
  'tu-modelo-personalizado': {
    temperature: 0.7,
    max_tokens: 2000,
    response_format: { type: 'json_object' }
  }
};
```

---

## ‚úÖ Verificar Modelo Activo

Los logs del servidor muestran qu√© modelo se est√° usando:

```
Groq API error: {
  message: '...',
  type: '...',
  model: 'llama-3.3-70b-versatile'  ‚Üê Modelo activo
}
```

---

**¬°Experimenta con diferentes modelos y encuentra el que mejor se adapte a tus necesidades! üöÄ**

**by JhonasDev** üåü
